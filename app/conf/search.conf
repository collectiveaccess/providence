#
# Search configuration
#

# Path to search index configuration file â€“ the file that
# determines what data gets indexed for search
search_indexing_config = <ca_conf_dir>/search_indexing.conf


# -------------------
# Solr plugin configuration
# -------------------
# Enter the home directory of the Solr here
search_solr_home_dir = /usr/local/solr/

# Enter the solr URL here
search_solr_url = http://localhost:9090/solr


# -------------------
# SqlSearch plugin configuration
# -------------------

# Set to 0 if you don't want search input stemmed (ie. suffixes removed) prior to search
# The plugin uses the English Snoball stemmer (http://snowball.tartarus.org/) and can give
# poor results with non-English content. If you are cataloguing non-English material you
# will probably want to turn this off.
search_sql_search_do_stemming = 1


# -------------------
# ElasticSearch plugin configuration
# -------------------

# Enter the elastic search base url here (without any index names)
search_elasticsearch_base_url = http://localhost:9200/

# This is the name of the ElasticSearch index used by CollectiveAccess.
# You probably don't need to change this unless you're using a single 
# ElasticSearch setup for multiple CollectiveAccess instances and/or
# other applications.
search_elasticsearch_index_name = collectiveaccess


# -------------------
# General search and indexing behavior (all plugins)
# -------------------
# Suffixes to add to searches if they conform to a listed regular expression
search_suffixes = {
#	[\d]+\.[0-9A-Za-z\.]* = *
}

# SearchIndexer replacement list
# These replacements are applied to the content before indexing is triggered.
# Use special value "nothing" if you want to delete certain characters/strings.
search_indexing_replacements = {
	"[" = nothing,
	"]" = nothing
}

# Regex character class used when indexing; values matched will be used as token delimiters
# (in other words, the search expression will be broken into words wherever the matched characters are)
indexing_tokenizer_regex = ^\pL\pN\pNd/_#\@\&\-

# Regex character class used when searching; values matched will be used as token delimiters
# (this is the same thing as indexing_tokenizer_regex except that it's used when searching rather than indexing)
search_tokenizer_regex = ^\pL\pN\pNd/_#\@\&\-

# List of regular expressions that if matched cause search input to be treated "as-is" - searched without processing
# This is useful for preventing tokenization of accession numbers and other values that rely upon punctuation being
# kept intact.
asis_regexes = [
	"^[\d]+[\.\-][A-Za-z0-9\.\-]+$",
	"[^\.]+\.[A-Za-z0-9]{3}$"
]